{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "from scripts.Agent import Episode, iRDPGAgent, PERBuffer, generate_demonstration_episodes, collect_episode\n",
    "from scripts.Env import POMDPTEnv, dt_policy, intraday_greedy_actions\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/INTC_1Min_2023-08-01_2024-01-31.csv')\n",
    "df = df[:len(df)//50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, env, agent, buffer):\n",
    "\n",
    "    actor_optim = optim.Adam(list(agent.actor_gru.parameters()) + list(agent.actor_fc.parameters()), lr=config[\"actor_lr\"])\n",
    "    critic_optim = optim.Adam(list(agent.critic_gru.parameters()) + list(agent.critic_fc.parameters()), lr=config[\"critic_lr\"])\n",
    "    \n",
    "\n",
    "    demo_episodes = generate_demonstration_episodes(env, \n",
    "                        n_episodes=config[\"min_demo_episodes\"])\n",
    "    \n",
    "    for episode in tqdm(demo_episodes, desc=\"Pre-filling buffer\"):\n",
    "        buffer.add_episode(episode)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in trange(config[\"epochs\"], desc=\"Training\"):\n",
    "\n",
    "        agent_episode = collect_episode(env, agent, add_noise=True)\n",
    "        buffer.add_episode(agent_episode)\n",
    "        \n",
    "        if len(buffer) < config[\"min_demo_episodes\"]:\n",
    "            continue\n",
    "            \n",
    "        batch, indices, weights = buffer.sample(config[\"batch_size\"])\n",
    "        \n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        new_priorities = []\n",
    "\n",
    "        # Process episodes\n",
    "        # Inside batch processing loop:\n",
    "        for episode in batch:\n",
    "            obs = episode.obs.unsqueeze(0)  # [1, T, obs_dim]\n",
    "            actions = episode.actions.unsqueeze(0)  # [1, T, action_dim]\n",
    "            rewards = episode.rewards.unsqueeze(0).unsqueeze(-1)  # [1, T, 1]\n",
    "            expert_acts = episode.expert_actions.unsqueeze(0)  # [1, T, action_dim]\n",
    "            dones = episode.dones.unsqueeze(0).unsqueeze(-1)  # [1, T, 1]\n",
    "\n",
    "            # Initialize hidden states at the start of each episode\n",
    "            h_actor, h_critic = None, None\n",
    "            h_actor_target, h_critic_target = None, None\n",
    "            \n",
    "            # Store per-timestep losses\n",
    "            episode_critic_loss = 0\n",
    "            episode_actor_loss = 0\n",
    "            \n",
    "            # Process each timestep\n",
    "            for t in range(obs.size(1)):\n",
    "                # Current timestep data\n",
    "                obs_t = obs[:, t:t+1, :]\n",
    "                action_t = actions[:, t:t+1, :]\n",
    "                reward_t = rewards[:, t:t+1, :]\n",
    "                done_t = dones[:, t:t+1, :]\n",
    "                expert_act_t = expert_acts[:, t:t+1, :]\n",
    "\n",
    "                # ----------------- Critic Update -----------------\n",
    "                with torch.no_grad():\n",
    "                    # Target Q-value\n",
    "                    _, target_q_t, h_actor_target, h_critic_target = agent.target_forward(\n",
    "                        obs_t, h_actor_target, h_critic_target\n",
    "                    )\n",
    "                    target_q = reward_t + (1 - done_t.float()) * config[\"gamma\"] * target_q_t\n",
    "\n",
    "                # Current Q-value\n",
    "                q_value_t, h_critic = agent.critic_forward(obs_t, action_t, h_critic)\n",
    "                \n",
    "                # Critic loss\n",
    "                critic_loss = F.mse_loss(q_value_t, target_q)\n",
    "                episode_critic_loss += critic_loss.item()\n",
    "\n",
    "                # ----------------- Actor Update -----------------\n",
    "                # Get action probabilities\n",
    "                action_probs_t, _, h_actor, _ = agent.forward(obs_t, h_actor, h_critic)\n",
    "                \n",
    "                # Policy gradient loss\n",
    "                actor_loss_pg = -q_value_t.mean()\n",
    "                \n",
    "                # Behavior cloning loss (with Q-filter)\n",
    "                with torch.no_grad():\n",
    "                    expert_q_t, _ = agent.critic_forward(obs_t, expert_act_t, h_critic)\n",
    "                \n",
    "                mask = (expert_q_t > q_value_t).float()\n",
    "                bc_loss = F.mse_loss(action_probs_t, expert_act_t.float()) * mask.mean()\n",
    "                \n",
    "                # Total actor loss\n",
    "                actor_loss = config[\"lambda1\"] * actor_loss_pg + config[\"lambda2\"] * bc_loss\n",
    "                episode_actor_loss += actor_loss.item()\n",
    "\n",
    "                # Detach hidden states for next step\n",
    "                h_actor = h_actor.detach() if h_actor is not None else None\n",
    "                h_critic = h_critic.detach() if h_critic is not None else None\n",
    "\n",
    "            # Store priorities (Eq 10)\n",
    "            priority = (episode_critic_loss + config[\"lambda0\"] * episode_actor_loss) / obs.size(1) + 1e-6\n",
    "            if episode.is_demo:\n",
    "                priority += config[\"eps_demo\"]\n",
    "            new_priorities.append(priority)\n",
    "\n",
    "            # Accumulate losses\n",
    "            critic_losses.append(episode_critic_loss / obs.size(1))\n",
    "            actor_losses.append(episode_actor_loss / obs.size(1))\n",
    "\n",
    "        # Update critic\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss = torch.stack(critic_losses).mean()\n",
    "        critic_loss.backward() \n",
    "        critic_optim.step()\n",
    "\n",
    "        # Update actor\n",
    "        actor_optim.zero_grad()\n",
    "        actor_loss = torch.stack(actor_losses).mean()\n",
    "        actor_loss.backward()\n",
    "        actor_optim.step()\n",
    "\n",
    "        # Update buffer priorities\n",
    "        buffer.update_priorities(indices, new_priorities)\n",
    "        \n",
    "        # Update target networks\n",
    "        agent._update_target_networks(config[\"tau\"])\n",
    "\n",
    "\n",
    "        if epoch % config['epochs']/1 == 0:\n",
    "            print(f\"Epoch {epoch} | Critic Loss: {critic_loss.item():.4f} | \"\n",
    "                  f\"Actor Loss: {actor_loss.item():.4f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(agent, filename=\"trained_irdpg.pth\"):\n",
    "    checkpoint = {\n",
    "        \"actor_gru\": agent.actor_gru.state_dict(),\n",
    "        \"actor_fc\": agent.actor_fc.state_dict(),\n",
    "        \"critic_gru\": agent.critic_gru.state_dict(),\n",
    "        \"critic_fc\": agent.critic_fc.state_dict(),\n",
    "        \"target_actor_gru\": agent.target_actor.state_dict(),\n",
    "        \"target_actor_fc\": agent.target_actor_fc.state_dict(),\n",
    "        \"target_critic_gru\": agent.target_critic.state_dict(),\n",
    "        \"target_critic_fc\": agent.target_critic_fc.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Model saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "df = pd.read_csv('data/INTC_1Min_2023-08-01_2024-01-31.csv')\n",
    "df = df[:len(df)//10]\n",
    "\n",
    "\n",
    "\n",
    "def train(config, env, agent, buffer):\n",
    "\n",
    "    actor_optim = optim.Adam(list(agent.actor_gru.parameters()) + list(agent.actor_fc.parameters()), lr=config[\"actor_lr\"])\n",
    "    critic_optim = optim.Adam(list(agent.critic_gru.parameters()) + list(agent.critic_fc.parameters()), lr=config[\"critic_lr\"])\n",
    "    \n",
    "\n",
    "    demo_episodes = generate_demonstration_episodes(env, \n",
    "                        n_episodes=config[\"min_demo_episodes\"])\n",
    "    \n",
    "    for episode in tqdm(demo_episodes, desc=\"Pre-filling buffer\"):\n",
    "        buffer.add_episode(episode)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in trange(config[\"epochs\"], desc=\"Training\"):\n",
    "\n",
    "        agent_episode = collect_episode(env, agent, add_noise=True)\n",
    "        buffer.add_episode(agent_episode)\n",
    "        \n",
    "        if len(buffer) < config[\"min_demo_episodes\"]:\n",
    "            continue\n",
    "            \n",
    "        batch, indices, weights = buffer.sample(config[\"batch_size\"])\n",
    "        \n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        new_priorities = []\n",
    "\n",
    "        # Process episodes\n",
    "        for episode in batch:\n",
    "            obs = episode.obs.unsqueeze(0)\n",
    "            actions = episode.actions.unsqueeze(0)\n",
    "            rewards = episode.rewards.unsqueeze(0).unsqueeze(-1)\n",
    "            expert_acts = episode.expert_actions.unsqueeze(0)\n",
    "            dones = episode.dones.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, target_q, _, _ = agent.target_forward(obs)\n",
    "                target_q = rewards + (1 - dones.half()) * config[\"gamma\"] * target_q\n",
    "\n",
    "            q_values, _ = agent.critic_forward(obs, actions)\n",
    "            \n",
    "            # Critic loss\n",
    "            critic_loss = F.mse_loss(q_values, target_q)\n",
    "            critic_losses.append(critic_loss)\n",
    "\n",
    "            with torch.no_grad(): # Detach from graph\n",
    "                expert_q, _ = agent.critic_forward(obs, expert_acts) \n",
    "                current_q = q_values.detach()\n",
    "\n",
    "            action_probs, _, _, _ = agent(obs)\n",
    "            \n",
    "            # Policy gradient loss\n",
    "            actor_loss = -current_q.mean()\n",
    "            \n",
    "            # Behavior cloning loss\n",
    "            mask = (expert_q > current_q).half()\n",
    "            bc_loss = F.mse_loss(action_probs, expert_acts.half()) * mask.mean()\n",
    "            \n",
    "            total_actor_loss = config[\"lambda1\"] * actor_loss + config[\"lambda2\"] * bc_loss\n",
    "            actor_losses.append(total_actor_loss)\n",
    "\n",
    "            # Update priorities\n",
    "            priority = critic_loss.item() + config[\"lambda0\"] * actor_loss.item()\n",
    "            if episode.is_demo:\n",
    "                priority += config[\"eps_demo\"]\n",
    "            new_priorities.append(priority)\n",
    "\n",
    "        # Update critic\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss = torch.stack(critic_losses).mean()\n",
    "        critic_loss.backward() \n",
    "        critic_optim.step()\n",
    "\n",
    "        # Update actor\n",
    "        actor_optim.zero_grad()\n",
    "        actor_loss = torch.stack(actor_losses).mean()\n",
    "        actor_loss.backward()\n",
    "        actor_optim.step()\n",
    "\n",
    "        # Update buffer priorities\n",
    "        buffer.update_priorities(indices, new_priorities)\n",
    "        \n",
    "        # Update target networks\n",
    "        agent._update_target_networks(config[\"tau\"])\n",
    "\n",
    "\n",
    "        if (epoch+1) % (config['epochs']/100) == 0:\n",
    "            print(f\"Epoch {epoch} | Critic Loss: {critic_loss.item():.4f} | \"\n",
    "                  f\"Actor Loss: {actor_loss.item():.4f}\")\n",
    "            save_model(agent, filename=f\"trained_irdpg_{epoch}.pth\")\n",
    "            \n",
    "\n",
    "def save_model(agent, filename=\"trained_irdpg.pth\"):\n",
    "    checkpoint = {\n",
    "        \"actor_gru\": agent.actor_gru.state_dict(),\n",
    "        \"actor_fc\": agent.actor_fc.state_dict(),\n",
    "        \"critic_gru\": agent.critic_gru.state_dict(),\n",
    "        \"critic_fc\": agent.critic_fc.state_dict(),\n",
    "        \"target_actor_gru\": agent.target_actor.state_dict(),\n",
    "        \"target_actor_fc\": agent.target_actor_fc.state_dict(),\n",
    "        \"target_critic_gru\": agent.target_critic.state_dict(),\n",
    "        \"target_critic_fc\": agent.target_critic_fc.state_dict(),\n",
    "        \"actor_optimizer\": agent.actor_optimizer.state_dict(),\n",
    "        \"critic_optimizer\": agent.critic_optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"\\nModel saved as {filename}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(agent, filename=\"trained_irdpg.pth\"):\n",
    "    checkpoint = torch.load(filename, map_location=agent.device)\n",
    "\n",
    "    agent.actor_gru.load_state_dict(checkpoint[\"actor_gru\"])\n",
    "    agent.actor_fc.load_state_dict(checkpoint[\"actor_fc\"])\n",
    "    agent.critic_gru.load_state_dict(checkpoint[\"critic_gru\"])\n",
    "    agent.critic_fc.load_state_dict(checkpoint[\"critic_fc\"])\n",
    "    agent.target_actor.load_state_dict(checkpoint[\"target_actor_gru\"])\n",
    "    agent.target_actor_fc.load_state_dict(checkpoint[\"target_actor_fc\"])\n",
    "    agent.target_critic.load_state_dict(checkpoint[\"target_critic_gru\"])\n",
    "    agent.target_critic_fc.load_state_dict(checkpoint[\"target_critic_fc\"])\n",
    "\n",
    "    return agent\n",
    "    print(f\"\\nModel loaded from {filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.1027,  0.0210,  0.0794, -0.0954,  0.0909,  0.0452,  0.0868, -0.1024,\n",
       "                        0.1211, -0.0515,  0.1004,  0.0420,  0.0295,  0.1051, -0.0607,  0.0284,\n",
       "                        0.0683, -0.0185,  0.1066, -0.0764,  0.0235, -0.0984,  0.0024, -0.1149,\n",
       "                        0.0137, -0.0884, -0.0858, -0.0363,  0.0203, -0.0447,  0.0693,  0.0247,\n",
       "                       -0.1353,  0.0586, -0.0826, -0.0429,  0.0173, -0.0338,  0.0196,  0.1142,\n",
       "                        0.0859, -0.0136, -0.0256,  0.0387,  0.0106,  0.0802,  0.0704, -0.0459,\n",
       "                       -0.0330, -0.0314,  0.0756,  0.1033, -0.0600, -0.0821,  0.0530,  0.0871,\n",
       "                       -0.0985, -0.0254, -0.1232, -0.0322, -0.0981,  0.0149,  0.0448,  0.0608],\n",
       "                      [-0.0724, -0.0845, -0.0466, -0.0092,  0.0117,  0.0819,  0.0978, -0.0568,\n",
       "                       -0.0615,  0.0178,  0.1019, -0.0615,  0.0828,  0.1118,  0.0128,  0.1117,\n",
       "                       -0.0800,  0.0719,  0.0636, -0.1357,  0.0503,  0.0391,  0.0387,  0.0494,\n",
       "                        0.1250,  0.0604, -0.1118, -0.0209,  0.1070, -0.0438,  0.0258,  0.0575,\n",
       "                       -0.0090, -0.0485,  0.0122,  0.0057, -0.0086, -0.0774,  0.0742, -0.0672,\n",
       "                       -0.0494, -0.0415, -0.0960,  0.0239, -0.1202,  0.0097, -0.0496,  0.0567,\n",
       "                       -0.0453, -0.1046,  0.0286, -0.0907, -0.0223, -0.1235,  0.1298,  0.0907,\n",
       "                       -0.0279, -0.0784, -0.0587,  0.1083, -0.0247, -0.0146,  0.0828,  0.0078]],\n",
       "                     device='cuda:0')),\n",
       "             ('bias', tensor([-0.0893, -0.0571], device='cuda:0'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"trained_irdpg_200.pth\")['actor_fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    \"epochs\": 500,          # Total training epochs\n",
    "    \"batch_size\": 32,       # Episodes per batch\n",
    "    \"gamma\": 0.99,          # Discount factor\n",
    "    \"tau\": 0.01,            # Target network update rate\n",
    "    \"lambda0\": 0.6,         # Priority weight\n",
    "    \"lambda1\": 0.8,         # Policy gradient weight\n",
    "    \"lambda2\": 0.2,         # BC loss weight\n",
    "    \"actor_lr\": 1e-4,       # Learning rates\n",
    "    \"critic_lr\": 1e-3,\n",
    "    \"eps_demo\": 0.1,        # Priority boost for demos\n",
    "    \"noise_std\": 0.1,       # Exploration noise\n",
    "    \"demo_ratio\": 0.3,      # Ratio of demo episodes in buffer\n",
    "    \"min_demo_episodes\": 10, # Min demo episodes to start\n",
    "    \"seq_len\": 60           # Match window_size\n",
    "}\n",
    "\n",
    "env = POMDPTEnv(df)\n",
    "agent = iRDPGAgent(obs_dim=env.observation_space.shape[0], device=device)\n",
    "buffer = PERBuffer(max_episodes=100)\n",
    "\n",
    "train(config, env, agent, buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = POMDPTEnv(df)\n",
    "agent = iRDPGAgent(obs_dim=env.observation_space.shape[0], device=device)\n",
    "# agent = load_model(agent, \"trained_irdpg_200.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'act'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics_df, cumulative_returns\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Usage Example\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m test_metrics, test_returns \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your test dataset\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m    124\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_rl/lib64/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 48\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[0;34m(env, agent, test_df, num_episodes, verbose)\u001b[0m\n\u001b[1;32m     44\u001b[0m max_drawdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Get deterministic action (no exploration noise)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     action, h_actor \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m(obs, h_actor, add_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Environment step\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     obs, reward, done, _ \u001b[38;5;241m=\u001b[39m test_env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'act'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_agent(env, agent, test_df, num_episodes=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on test data using paper's metrics\n",
    "    Args:\n",
    "        env: Training environment template (for config)\n",
    "        agent: Trained iRDPG agent\n",
    "        test_df: Test dataset DataFrame (OHLC + timestamp)\n",
    "        num_episodes: Number of evaluation episodes\n",
    "    Returns:\n",
    "        metrics_df: DataFrame with evaluation metrics\n",
    "        cumulative_returns: List of account value trajectories\n",
    "    \"\"\"\n",
    "    # Create test environment with same params as training\n",
    "    test_env = POMDPTEnv(\n",
    "        df=test_df,\n",
    "        window_size=env.window_size,\n",
    "        initial_balance=env.initial_balance,\n",
    "        transaction_cost=env.transaction_cost,\n",
    "        slippage=env.slippage\n",
    "    )\n",
    "    \n",
    "    # Storage for results\n",
    "    all_total_returns = []\n",
    "    all_sharpe_ratios = []\n",
    "    all_volatilities = []\n",
    "    all_max_drawdowns = []\n",
    "    cumulative_returns = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        # Initialize episode\n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        h_actor = None\n",
    "        \n",
    "        # Track metrics\n",
    "        daily_returns = []\n",
    "        account_values = [test_env.initial_balance]\n",
    "        peak_value = test_env.initial_balance\n",
    "        max_drawdown = 0\n",
    "\n",
    "        while not done:\n",
    "            # Get deterministic action (no exploration noise)\n",
    "            action, h_actor = agent.act(obs, h_actor, add_noise=False)\n",
    "            \n",
    "            # Environment step\n",
    "            obs, reward, done, _ = test_env.step(action)\n",
    "            \n",
    "            # Track performance\n",
    "            current_value = test_env.balance\n",
    "            daily_returns.append(test_env.cumulative_profit)\n",
    "            account_values.append(current_value)\n",
    "            \n",
    "            # Update max drawdown\n",
    "            peak_value = max(peak_value, current_value)\n",
    "            current_drawdown = (peak_value - current_value) / peak_value\n",
    "            max_drawdown = max(max_drawdown, current_drawdown)\n",
    "\n",
    "        # Calculate metrics (paper's definitions)\n",
    "        returns = np.diff(account_values) / account_values[:-1]  # Daily returns\n",
    "        \n",
    "        # 1. Total Return\n",
    "        total_return = (account_values[-1] - account_values[0]) / account_values[0]\n",
    "        \n",
    "        # 2. Sharpe Ratio (risk-free rate = 0)\n",
    "        sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-9)  # Avoid division by zero\n",
    "        \n",
    "        # 3. Volatility\n",
    "        volatility = np.std(returns)\n",
    "        \n",
    "        # 4. Max Drawdown\n",
    "        max_drawdown_pct = max_drawdown * 100\n",
    "        \n",
    "        # Store results\n",
    "        all_total_returns.append(total_return)\n",
    "        all_sharpe_ratios.append(sharpe_ratio)\n",
    "        all_volatilities.append(volatility)\n",
    "        all_max_drawdowns.append(max_drawdown_pct)\n",
    "        cumulative_returns.append(account_values)\n",
    "\n",
    "        # Plotting\n",
    "        if verbose:\n",
    "            plt.figure(figsize=(10,6))\n",
    "            plt.plot(account_values, label='iRDPG Agent')\n",
    "            plt.title(f\"Evaluation Episode {ep+1}\\nTotal Return: {total_return*100:.1f}%\")\n",
    "            plt.xlabel(\"Time Steps\")\n",
    "            plt.ylabel(\"Account Value ($)\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    # Aggregate metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Total Return (%)': np.array(all_total_returns) * 100,\n",
    "        'Sharpe Ratio': all_sharpe_ratios,\n",
    "        'Volatility': all_volatilities,\n",
    "        'Max Drawdown (%)': all_max_drawdowns\n",
    "    })\n",
    "    \n",
    "    # Add averages\n",
    "    metrics_df.loc['Mean'] = metrics_df.mean()\n",
    "    metrics_df.loc['Std'] = metrics_df.std()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nFinal Evaluation Metrics:\")\n",
    "        print(metrics_df.round(2).to_markdown())\n",
    "        print(\"\\nKey Metrics Across Episodes:\")\n",
    "        print(f\"Average Total Return: {np.mean(all_total_returns)*100:.1f}%\")\n",
    "        print(f\"Average Sharpe Ratio: {np.mean(all_sharpe_ratios):.2f}\")\n",
    "        print(f\"Average Volatility: {np.mean(all_volatilities):.4f}\")\n",
    "        print(f\"Average Max Drawdown: {np.mean(all_max_drawdowns):.1f}%\")\n",
    "\n",
    "    return metrics_df, cumulative_returns\n",
    "\n",
    "# Usage Example\n",
    "test_metrics, test_returns = evaluate_agent(\n",
    "    env=env, \n",
    "    agent=agent,\n",
    "    test_df=df,  # Your test dataset\n",
    "    num_episodes=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
