{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "from scripts.Agent import iRDPGAgent\n",
    "from scripts.PRBuffer import Episode, PRBuffer, generate_demonstration_episodes, collect_episode\n",
    "from scripts.Env import POMDPTEnv, dt_policy, intraday_greedy_actions\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/INTC_1Min_2024-02-01_2025-02-01.csv')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "df = df.loc['2024-02-01':'2024-05-01']\n",
    "df.to_csv('data/train_three_months.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_three_months.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = POMDPTEnv(df, window_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24242"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intraday_greedy_actions(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(12121)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(agent, filename=\"trained_irdpg.pth\"):\n",
    "    checkpoint = torch.load(filename, map_location=agent.device)\n",
    "\n",
    "    agent.actor_gru.load_state_dict(checkpoint[\"actor_gru\"])\n",
    "    agent.actor_fc.load_state_dict(checkpoint[\"actor_fc\"])\n",
    "    agent.critic_gru.load_state_dict(checkpoint[\"critic_gru\"])\n",
    "    agent.critic_fc.load_state_dict(checkpoint[\"critic_fc\"])\n",
    "    agent.target_actor.load_state_dict(checkpoint[\"target_actor_gru\"])\n",
    "    agent.target_actor_fc.load_state_dict(checkpoint[\"target_actor_fc\"])\n",
    "    agent.target_critic.load_state_dict(checkpoint[\"target_critic_gru\"])\n",
    "    agent.target_critic_fc.load_state_dict(checkpoint[\"target_critic_fc\"])\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'int' and 'datetime.date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mPOMDPTEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m iRDPGAgent(obs_dim\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m load_model(agent, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/01/trained_irdpg_500.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/adaptive-quant-trading/scripts/Env.py:53\u001b[0m, in \u001b[0;36mPOMDPTEnv.__init__\u001b[0;34m(self, df, window_size, initial_balance, transaction_cost, slippage, eta, k1, k2)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_df(df)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorize()\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adaptive-quant-trading/scripts/Env.py:143\u001b[0m, in \u001b[0;36mPOMDPTEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_week \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_weeks)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweek_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_week)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 143\u001b[0m valid_week_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweek_indices[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweek_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst_valid_idx\u001b[49m]\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_week_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Fallback to first valid week\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_week \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_weeks[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'int' and 'datetime.date'"
     ]
    }
   ],
   "source": [
    "env = POMDPTEnv(df, window_size=1) \n",
    "agent = iRDPGAgent(obs_dim=env.observation_space.shape[0], device=device)\n",
    "agent = load_model(agent, filename=\"models/01/trained_irdpg_500.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 3 episodes:\n",
      "Average Total Return: -5.32%\n",
      "Average Sharpe Ratio: 0.03\n",
      "Average Volatility: 239.9122\n",
      "Average Max Drawdown: 2.66%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_returns': [np.float64(-0.026580847810374467),\n",
       "  np.float64(-0.053161695620749495),\n",
       "  np.float64(-0.07974254343112411)],\n",
       " 'sharpe_ratios': [np.float64(0.026412928732093707),\n",
       "  np.float64(0.026412928732093707),\n",
       "  np.float64(0.026412928732093707)],\n",
       " 'volatilities': [np.float64(239.91218113917725),\n",
       "  np.float64(239.91218113917725),\n",
       "  np.float64(239.91218113917725)],\n",
       " 'max_drawdowns': [np.float64(0.026580847810348206),\n",
       "  np.float64(0.026580847810348206),\n",
       "  np.float64(0.026580847810348206)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(env, agent, model_path, n_episodes=10):\n",
    "    # Load trained model\n",
    "    agent = load_model(agent, model_path)\n",
    "    agent.eval()\n",
    "    \n",
    "    metrics = {\n",
    "        \"total_returns\": [],\n",
    "        \"sharpe_ratios\": [],\n",
    "        \"volatilities\": [],\n",
    "        \"max_drawdowns\": []\n",
    "    }\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        episode_returns = []\n",
    "        episode_values = []\n",
    "        obs = env.reset()\n",
    "        h_actor = None\n",
    "        done = False\n",
    "        peak_value = env.initial_balance\n",
    "        max_drawdown = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action, h_actor = agent.act(obs, h_actor, add_noise=False)\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Track metrics\n",
    "            current_value = env.balance\n",
    "            episode_returns.append(reward)\n",
    "            episode_values.append(current_value)\n",
    "            \n",
    "            # Calculate drawdown\n",
    "            peak_value = max(peak_value, current_value)\n",
    "            drawdown = (peak_value - current_value) / peak_value\n",
    "            max_drawdown = max(max_drawdown, drawdown)\n",
    "\n",
    "        # Episode metrics\n",
    "        returns = np.array(episode_returns)\n",
    "        metrics[\"total_returns\"].append(env.cumulative_profit / env.initial_balance)\n",
    "        metrics[\"sharpe_ratios\"].append(returns.mean() / (returns.std() + 1e-9))\n",
    "        metrics[\"volatilities\"].append(returns.std())\n",
    "        metrics[\"max_drawdowns\"].append(max_drawdown)\n",
    "    \n",
    "    # Aggregate results\n",
    "    print(f\"Evaluation over {n_episodes} episodes:\")\n",
    "    print(f\"Average Total Return: {np.mean(metrics['total_returns']):.2%}\")\n",
    "    print(f\"Average Sharpe Ratio: {np.mean(metrics['sharpe_ratios']):.2f}\")\n",
    "    print(f\"Average Volatility: {np.mean(metrics['volatilities']):.4f}\")\n",
    "    print(f\"Average Max Drawdown: {np.mean(metrics['max_drawdowns']):.2%}\")\n",
    "    \n",
    "    # Plot cumulative returns\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(min(3, n_episodes)):  # Plot first 3 episodes\n",
    "        plt.plot(np.cumsum(episode_returns[i]), label=f\"Episode {i+1}\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Cumulative Return\")\n",
    "    plt.title(\"Evaluation: Cumulative Returns\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"evaluation_returns.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "evaluate(env, agent, 'models/trained_irdpg_500.pth', n_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def evaluate(agent, env, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluate the agent's performance over multiple episodes.\n",
    "    \n",
    "    Args:\n",
    "        agent (iRDPGAgent): Trained agent\n",
    "        env (TradingEnv): Trading environment\n",
    "        num_episodes (int): Number of evaluation episodes\n",
    "        \n",
    "    Returns:\n",
    "        dict: Aggregated evaluation metrics\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    metrics = {\n",
    "        'cumulative_returns': [],\n",
    "        'annualized_sharpe': [],\n",
    "        'max_drawdowns': [],\n",
    "        'win_rate': [],\n",
    "        'profit_factor': [],\n",
    "        'num_trades': [],\n",
    "        'action_distribution': [],\n",
    "        'portfolio_values': []\n",
    "    }\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        h_actor = None\n",
    "        done = False\n",
    "        \n",
    "        # Episode tracking\n",
    "        portfolio = [env.initial_balance]\n",
    "        positions = []\n",
    "        actions = []\n",
    "        trade_returns = []\n",
    "        current_balance = env.initial_balance\n",
    "        prev_position = env.position\n",
    "        num_trades = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action, h_actor = agent.act(obs, h_actor, add_noise=False)\n",
    "            \n",
    "            # Store action and position\n",
    "            actions.append(action.cpu().numpy())\n",
    "            positions.append(env.position)\n",
    "            \n",
    "            # Execute step\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Calculate returns\n",
    "            new_balance = env.balance\n",
    "            trade_return = new_balance - current_balance\n",
    "            trade_returns.append(trade_return)\n",
    "            portfolio.append(new_balance)\n",
    "            \n",
    "            # Track position changes\n",
    "            if env.position != prev_position:\n",
    "                num_trades += 1\n",
    "            prev_position = env.position\n",
    "            \n",
    "            current_balance = new_balance\n",
    "            obs = next_obs\n",
    "\n",
    "        # Calculate metrics\n",
    "        returns = np.array(trade_returns)\n",
    "        portfolio = np.array(portfolio)\n",
    "        \n",
    "        # Cumulative return\n",
    "        cumulative_return = (portfolio[-1] / portfolio[0] - 1) * 100\n",
    "        \n",
    "        # Sharpe ratio (annualized)\n",
    "        if len(returns) > 1:\n",
    "            sharpe = (np.mean(returns) / np.std(returns)) * np.sqrt(252)\n",
    "        else:\n",
    "            sharpe = 0.0\n",
    "            \n",
    "        # Max drawdown\n",
    "        peak = np.maximum.accumulate(portfolio)\n",
    "        drawdown = (peak - portfolio) / peak\n",
    "        max_drawdown = np.max(drawdown) * 100 if len(drawdown) > 0 else 0.0\n",
    "        \n",
    "        # Win rate and profit factor\n",
    "        wins = returns[returns > 0]\n",
    "        losses = returns[returns < 0]\n",
    "        win_rate = len(wins) / len(returns) * 100 if len(returns) > 0 else 0.0\n",
    "        profit_factor = np.sum(wins) / np.abs(np.sum(losses)) if len(losses) > 0 else np.inf\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics['cumulative_returns'].append(cumulative_return)\n",
    "        metrics['annualized_sharpe'].append(sharpe)\n",
    "        metrics['max_drawdowns'].append(max_drawdown)\n",
    "        metrics['win_rate'].append(win_rate)\n",
    "        metrics['profit_factor'].append(profit_factor)\n",
    "        metrics['num_trades'].append(num_trades)\n",
    "        metrics['action_distribution'].append(np.mean(actions, axis=0))\n",
    "        metrics['portfolio_values'].append(portfolio)\n",
    "\n",
    "    # Generate report\n",
    "    report = {\n",
    "        'cumulative_return (%)': f\"{np.mean(metrics['cumulative_returns']):.2f} ± {np.std(metrics['cumulative_returns']):.2f}\",\n",
    "        'sharpe_ratio': f\"{np.nanmean(metrics['annualized_sharpe']):.2f} ± {np.nanstd(metrics['annualized_sharpe']):.2f}\",\n",
    "        'max_drawdown (%)': f\"{np.mean(metrics['max_drawdowns']):.2f} ± {np.std(metrics['max_drawdowns']):.2f}\",\n",
    "        'win_rate (%)': f\"{np.mean(metrics['win_rate']):.2f} ± {np.std(metrics['win_rate']):.2f}\",\n",
    "        'profit_factor': f\"{np.mean([p for p in metrics['profit_factor'] if p != np.inf]):.2f}\",\n",
    "        'avg_trades_per_episode': f\"{np.mean(metrics['num_trades']):.1f} ± {np.std(metrics['num_trades']):.1f}\",\n",
    "    }\n",
    "\n",
    "    # Plotting\n",
    "    plot_equity_curves(metrics['portfolio_values'])\n",
    "    plot_action_distribution(metrics['action_distribution'])\n",
    "\n",
    "    return report\n",
    "\n",
    "def plot_equity_curves(portfolio_values, num_curves=5):\n",
    "    \"\"\"Plot first few equity curves\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, pv in enumerate(portfolio_values[:num_curves]):\n",
    "        plt.plot(pv / pv[0], label=f'Episode {i+1}')\n",
    "    plt.title('Normalized Equity Curves')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Portfolio Value (Multiple of Initial)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('equity_curves.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_action_distribution(action_distributions):\n",
    "    \"\"\"Plot average action probabilities\"\"\"\n",
    "    avg_actions = np.mean(action_distributions, axis=0)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(['Long', 'Short'], avg_actions)\n",
    "    plt.title('Average Action Distribution')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.savefig('action_distribution.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_model(agent, filename=\"models/01/trained_irdpg_500.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cumulative_return (%)': '2.65 ± 0.01',\n",
       " 'sharpe_ratio': '115.90 ± 16.05',\n",
       " 'max_drawdown (%)': '0.00 ± 0.00',\n",
       " 'win_rate (%)': '99.75 ± 0.16',\n",
       " 'profit_factor': '623.75',\n",
       " 'avg_trades_per_episode': '20.2 ± 21.0'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(agent, env, num_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
