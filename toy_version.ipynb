{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "from scripts.Agent import Episode, iRDPGAgent, PERBuffer, generate_demonstration_episodes, collect_episode\n",
    "from scripts.Env import POMDPTEnv, dt_policy, intraday_greedy_actions\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/INTC_1Min_2023-08-01_2024-01-31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, env, agent, buffer):\n",
    "\n",
    "    actor_optim = optim.Adam(list(agent.actor_gru.parameters()) + list(agent.actor_fc.parameters()), lr=config[\"actor_lr\"])\n",
    "    critic_optim = optim.Adam(list(agent.critic_gru.parameters()) + list(agent.critic_fc.parameters()), lr=config[\"critic_lr\"])\n",
    "    \n",
    "\n",
    "    demo_episodes = generate_demonstration_episodes(env, \n",
    "                        n_episodes=config[\"min_demo_episodes\"])\n",
    "    \n",
    "    for episode in tqdm(demo_episodes, desc=\"Pre-filling buffer\"):\n",
    "        buffer.add_episode(episode)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in trange(config[\"epochs\"], desc=\"Training\"):\n",
    "\n",
    "        agent_episode = collect_episode(env, agent, add_noise=True)\n",
    "        buffer.add_episode(agent_episode)\n",
    "        \n",
    "        if len(buffer) < config[\"min_demo_episodes\"]:\n",
    "            continue\n",
    "            \n",
    "        batch, indices, weights = buffer.sample(config[\"batch_size\"])\n",
    "        \n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        new_priorities = []\n",
    "\n",
    "        # Process episodes\n",
    "        for episode in batch:\n",
    "            obs = episode.obs.unsqueeze(0)\n",
    "            actions = episode.actions.unsqueeze(0)\n",
    "            rewards = episode.rewards.unsqueeze(0).unsqueeze(-1)\n",
    "            expert_acts = episode.expert_actions.unsqueeze(0)\n",
    "            dones = episode.dones.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, target_q, _, _ = agent.target_forward(obs)\n",
    "                target_q = rewards + (1 - dones.float()) * config[\"gamma\"] * target_q\n",
    "\n",
    "            _, q_values, _, _ = agent(obs)\n",
    "            \n",
    "            # Critic loss\n",
    "            critic_loss = F.mse_loss(q_values, target_q)\n",
    "            critic_losses.append(critic_loss)\n",
    "\n",
    "            with torch.no_grad(): # Detach from graph\n",
    "                expert_q, _ = agent.critic_forward(obs, expert_acts) \n",
    "                current_q = q_values.detach()\n",
    "\n",
    "            action_probs, _, _, _ = agent(obs)\n",
    "            \n",
    "            # Policy gradient loss\n",
    "            actor_loss = -current_q.mean()\n",
    "            \n",
    "            # Behavior cloning loss\n",
    "            mask = (expert_q > current_q).float()\n",
    "            bc_loss = F.mse_loss(action_probs, expert_acts.float()) * mask.mean()\n",
    "            \n",
    "            total_actor_loss = config[\"lambda1\"] * actor_loss + config[\"lambda2\"] * bc_loss\n",
    "            actor_losses.append(total_actor_loss)\n",
    "\n",
    "            # Update priorities\n",
    "            priority = critic_loss.item() + config[\"lambda0\"] * actor_loss.item() + 1e-6\n",
    "            if episode.is_demo:\n",
    "                priority += config[\"eps_demo\"]\n",
    "            new_priorities.append(priority)\n",
    "\n",
    "        # Update critic\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss = torch.stack(critic_losses).mean()\n",
    "        critic_loss.backward() \n",
    "        critic_optim.step()\n",
    "\n",
    "        # Update actor\n",
    "        actor_optim.zero_grad()\n",
    "        actor_loss = torch.stack(actor_losses).mean()\n",
    "        actor_loss.backward()\n",
    "        actor_optim.step()\n",
    "\n",
    "        # Update buffer priorities\n",
    "        buffer.update_priorities(indices, new_priorities)\n",
    "        \n",
    "        # Update target networks\n",
    "        agent._update_target_networks(config[\"tau\"])\n",
    "\n",
    "\n",
    "        if epoch % config['epochs']/10 == 0:\n",
    "            print(f\"Epoch {epoch} | Critic Loss: {critic_loss.item():.4f} | \"\n",
    "                  f\"Actor Loss: {actor_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 1_000,          # Total training epochs\n",
    "    \"batch_size\": 32,       # Episodes per batch\n",
    "    \"gamma\": 0.99,          # Discount factor\n",
    "    \"tau\": 0.01,            # Target network update rate\n",
    "    \"lambda0\": 0.5,         # Priority weight\n",
    "    \"lambda1\": 0.8,         # Policy gradient weight\n",
    "    \"lambda2\": 0.2,         # BC loss weight\n",
    "    \"actor_lr\": 1e-4,       # Learning rates\n",
    "    \"critic_lr\": 1e-3,\n",
    "    \"eps_demo\": 0.1,        # Priority boost for demos\n",
    "    \"noise_std\": 0.1,       # Exploration noise\n",
    "    \"demo_ratio\": 0.3,      # Ratio of demo episodes in buffer\n",
    "    \"min_demo_episodes\": 50, # Min demo episodes to start\n",
    "    \"seq_len\": 60           # Match window_size\n",
    "}\n",
    "\n",
    "env = POMDPTEnv(df)\n",
    "agent = iRDPGAgent(obs_dim=env.observation_space.shape[0], device=device)\n",
    "buffer = PERBuffer(max_episodes=200)\n",
    "\n",
    "train(config, env, agent, buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_agent(env, agent, test_df, num_episodes=5, plot=True):\n",
    "    # Create test environment\n",
    "    test_env = POMDPTEnv(test_df, window_size=env.window_size)\n",
    "    \n",
    "    # Storage for metrics\n",
    "    all_total_returns = []\n",
    "    all_sharpe_ratios = []\n",
    "    all_volatilities = []\n",
    "    all_max_drawdowns = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        # Initialize episode\n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        h_actor = None\n",
    "        \n",
    "        # Track values for metrics\n",
    "        daily_returns = []\n",
    "        account_values = [test_env.initial_balance]\n",
    "        peak = test_env.initial_balance\n",
    "        max_drawdown = 0\n",
    "\n",
    "        while not done:\n",
    "            # Get agent action (no exploration noise)\n",
    "            action, h_actor = agent.act(obs, h_actor, add_noise=False)\n",
    "            \n",
    "            # Environment step\n",
    "            obs, reward, done, info = test_env.step(action)\n",
    "            \n",
    "            # Track daily performance\n",
    "            daily_returns.append(test_env.cumulative_profit)\n",
    "            current_value = test_env.balance\n",
    "            account_values.append(current_value)\n",
    "            \n",
    "            # Update max drawdown\n",
    "            peak = max(peak, current_value)\n",
    "            trough = current_value\n",
    "            drawdown = (peak - trough) / peak\n",
    "            max_drawdown = max(max_drawdown, drawdown)\n",
    "\n",
    "        # Calculate metrics\n",
    "        returns = np.diff(daily_returns)\n",
    "        \n",
    "        # Total Return\n",
    "        total_return = (account_values[-1] - account_values[0]) / account_values[0]\n",
    "        \n",
    "        # Sharpe Ratio (assuming risk-free rate = 0)\n",
    "        sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-9)\n",
    "        \n",
    "        # Volatility\n",
    "        volatility = np.std(returns)\n",
    "        \n",
    "        # Store metrics\n",
    "        all_total_returns.append(total_return)\n",
    "        all_sharpe_ratios.append(sharpe_ratio)\n",
    "        all_volatilities.append(volatility)\n",
    "        all_max_drawdowns.append(max_drawdown)\n",
    "\n",
    "        # Plot cumulative returns\n",
    "        if plot:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(account_values, label='iRDPG')\n",
    "            plt.title(f\"Evaluation Episode {ep+1} - Cumulative Account Value\")\n",
    "            plt.xlabel(\"Time Steps\")\n",
    "            plt.ylabel(\"Account Value ($)\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    # Aggregate results\n",
    "    metrics = {\n",
    "        'Total Return (%)': [f\"{tr*100:.2f}%\" for tr in all_total_returns],\n",
    "        'Sharpe Ratio': [f\"{sr:.3f}\" for sr in all_sharpe_ratios],\n",
    "        'Volatility': [f\"{vol:.5f}\" for vol in all_volatilities],\n",
    "        'Max Drawdown (%)': [f\"{mdd*100:.2f}%\" for mdd in all_max_drawdowns]\n",
    "    }\n",
    "    \n",
    "    # Print results table\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(pd.DataFrame(metrics))\n",
    "    \n",
    "    return {\n",
    "        'total_returns': all_total_returns,\n",
    "        'sharpe_ratios': all_sharpe_ratios,\n",
    "        'volatilities': all_volatilities,\n",
    "        'max_drawdowns': all_max_drawdowns,\n",
    "        'account_values': account_values\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "test_results = evaluate_agent(env, agent, df, num_episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
