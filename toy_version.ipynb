{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gym import spaces\n",
    "from gym_trading_env.environments import TradingEnv\n",
    "\n",
    "from scripts import *\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/INTC_1Min_2023-08-01_2024-01-31.csv')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "df = df.iloc[:len(df) // 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_iRDPG(df, window_size=60, hidden_dim=64,\n",
    "#                 train_episodes=500, batch_size=8, gamma=0.99,\n",
    "#                 tau=0.01, expert_demos=None):\n",
    "    \n",
    "#     env = POMDPTEnv(df, window_size=window_size)\n",
    "#     obs_dim = env.observation_space.shape[0]\n",
    "#     act_dim = env.action_space.n\n",
    "    \n",
    "#     agent = iRDPGAgent(obs_dim, act_dim, hidden_dim).to(device)\n",
    "#     target_agent = iRDPGAgent(obs_dim, act_dim, hidden_dim).to(device)\n",
    "#     target_agent.load_state_dict(agent.state_dict())\n",
    "    \n",
    "#     # Create a replay buffer\n",
    "#     buffer = RBuffer(max_episodes=1000)\n",
    "#     print(f\"Initialized the environment: window_size={window_size}, \\n \\\n",
    "#           the agent: observation dimension: {obs_dim}, action dimension: {act_dim}, \\n \\\n",
    "#           and replay buffer.\")\n",
    "    \n",
    "#     # Collect some demonstration episodes\n",
    "#     for ep in expert_demos:\n",
    "#         buffer.add_episode(ep)\n",
    "#     print(f\"Collected {len(expert_demos)} demonstration episodes.\")\n",
    "    \n",
    "#     # Create \"prophetic\" array for entire df:\n",
    "#     prophecy = intraday_greedy_actions(df, window_size=window_size, device=device)\n",
    "#     print(f\"Created the 'prophetic' array for the entire dataset.\")\n",
    "    \n",
    "#     # Attach them to each Episode so we can do Q-filter BC later\n",
    "#     for ep in tqdm(buffer.episodes, desc=\"Attaching Prophetic Actions\"):\n",
    "#         ep.expert_actions = []\n",
    "#         # naive mapping: the environment index starts at window_size for step=0\n",
    "#         # so step i -> global index = window_size + i\n",
    "#         idx_global = window_size\n",
    "#         for t in range(len(ep.obs)):\n",
    "#             if idx_global < len(prophecy):\n",
    "#                 ep.expert_actions.append(prophecy[idx_global])\n",
    "#             else:\n",
    "#                 ep.expert_actions.append(0)\n",
    "#             idx_global += 1\n",
    "#     print(f\"Attached the 'prophetic' actions to demonstration episodes.\")\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(agent.parameters(), lr=1e-3)\n",
    "#     ce = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     def soft_update(net, net_targ, tau):\n",
    "#         for p, p_targ in zip(net.parameters(), net_targ.parameters()):\n",
    "#             p_targ.data.copy_(tau * p.data + (1.0 - tau) * p_targ.data)\n",
    "    \n",
    "#     def update_agent(batch_eps):\n",
    "#         \"\"\"\n",
    "#         Unroll entire episodes on GPU for BPTT.\n",
    "#         \"\"\"\n",
    "#         total_loss = 0.0\n",
    "#         agent.train()\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         for ep in tqdm(batch_eps, desc=\"Updating agent\"):\n",
    "#             obs_seq = torch.tensor(ep.obs, dtype=torch.float, device=device)\n",
    "#             actions_seq = torch.tensor(ep.actions, dtype=torch.long, device=device)\n",
    "#             rewards_seq = torch.tensor(ep.rewards, dtype=torch.float, device=device)\n",
    "#             exp_seq = torch.tensor(ep.expert_actions, dtype=torch.long, device=device)\n",
    "            \n",
    "#             # shape => [1, T, obs_dim]\n",
    "#             obs_seq = obs_seq.unsqueeze(0) \n",
    "            \n",
    "#             logits, Q_vals, _, _ = agent(obs_seq, actions_seq)  # [1,T,act_dim]\n",
    "#             logits = logits[0]  # [T,act_dim]\n",
    "#             Q_vals = Q_vals[0] # [T,act_dim]\n",
    "#             T_len = len(actions_seq)\n",
    "            \n",
    "#             # Critic loss: basic 1-step TD\n",
    "#             td_errors = []\n",
    "#             for t in range(T_len):\n",
    "#                 q_sa = Q_vals[t, actions_seq[t]]\n",
    "#                 if t < T_len-1 and not ep.done_flags[t]:\n",
    "#                     # next action from agent policy\n",
    "#                     next_a = logits[t+1].argmax().item()\n",
    "#                     q_next = Q_vals[t+1, next_a].detach()\n",
    "#                     target = rewards_seq[t] + gamma * q_next\n",
    "#                 else:\n",
    "#                     target = rewards_seq[t]\n",
    "#                 td_errors.append(target - q_sa)\n",
    "            \n",
    "#             td_errors = torch.stack(td_errors)\n",
    "#             critic_loss = (td_errors**2).mean()\n",
    "            \n",
    "#             # Actor loss: - Q(s, a_pi)\n",
    "#             pi = torch.softmax(logits, dim=-1)\n",
    "#             Q_expected = (pi * Q_vals).sum(dim=-1)\n",
    "#             actor_loss = - Q_expected.mean()\n",
    "            \n",
    "#             # Behavior cloning with Q-filter\n",
    "#             a_agent = logits.argmax(dim=-1)\n",
    "#             Q_expert = Q_vals[torch.arange(T_len), exp_seq]\n",
    "#             Q_agent  = Q_vals[torch.arange(T_len), a_agent]\n",
    "#             mask = (Q_expert > Q_agent).float()\n",
    "            \n",
    "#             bc_loss_t = ce(logits, exp_seq)  # cross-entropy stepwise\n",
    "#             bc_loss   = (bc_loss_t * mask).mean()\n",
    "            \n",
    "#             ep_loss = critic_loss + actor_loss + bc_loss\n",
    "#             ep_loss.backward(retain_graph=True)\n",
    "#             total_loss += ep_loss.item()\n",
    "        \n",
    "#         nn.utils.clip_grad_norm_(agent.parameters(), 5.0)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         soft_update(agent, target_agent, tau)\n",
    "        \n",
    "#         return total_loss / len(batch_eps)\n",
    "    \n",
    "#     # main training loop\n",
    "#     update_trange = trange(train_episodes, desc=\"Training\")\n",
    "#     for ep_i in update_trange:\n",
    "#         new_ep = collect_episode(env, agent=agent, noise=True)\n",
    "#         # attach prophecy to new episode\n",
    "#         new_ep.expert_actions = []\n",
    "#         idx_global = window_size\n",
    "#         for t in range(len(new_ep.obs)):\n",
    "#             if idx_global < len(prophecy):\n",
    "#                 new_ep.expert_actions.append(prophecy[idx_global])\n",
    "#             else:\n",
    "#                 new_ep.expert_actions.append(0)\n",
    "#             idx_global += 1\n",
    "        \n",
    "#         buffer.add_episode(new_ep)\n",
    "        \n",
    "#         if len(buffer) >= batch_size:\n",
    "#             batch_eps = buffer.sample(batch_size)\n",
    "#             loss_val = update_agent(batch_eps)\n",
    "#         else:\n",
    "#             loss_val = 0.0\n",
    "#         update_trange.set_postfix(loss=loss_val)\n",
    "        \n",
    "#         print(f\"Episode {ep_i+1}/{train_episodes}, Loss={loss_val:.4f}\")\n",
    "    \n",
    "#     return agent, target_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_iRDPG(\n",
    "#     df,\n",
    "#     n_train_episodes=100,\n",
    "#     n_demo_episodes=20,\n",
    "#     window_size=60,\n",
    "#     batch_size=4,\n",
    "#     train_iterations_per_episode=10,\n",
    "#     gamma=0.99,\n",
    "#     lr_actor=1e-4,\n",
    "#     lr_critic=1e-3,\n",
    "#     expert_demos=None,\n",
    "#     device=\"cuda\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Example training loop for iRDPG.\n",
    "\n",
    "#     :param df:          (pandas.DataFrame) containing columns ['open','high','low','close'] for the environment\n",
    "#     :param n_train_episodes: (int) total training episodes\n",
    "#     :param n_demo_episodes:  (int) how many demonstration episodes to collect from Dual Thrust\n",
    "#     :param window_size: (int) lookback window for environment\n",
    "#     :param demo_k1:     (float) parameter K1 for Dual Thrust\n",
    "#     :param demo_k2:     (float) parameter K2 for Dual Thrust\n",
    "#     :param batch_size:  (int) batch size for sampling episodes from replay\n",
    "#     :param train_iterations_per_episode: (int) how many gradient updates to do after each collection\n",
    "#     :param gamma:       (float) discount factor\n",
    "#     :param lr_actor:    (float) learning rate for actor\n",
    "#     :param lr_critic:   (float) learning rate for critic\n",
    "#     :param device:      (str) \"cuda\" or \"cpu\"\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 1) Create the environment\n",
    "#     env = POMDPTEnv(df, window_size=window_size)\n",
    "\n",
    "#     # 2) Create the agent (iRDPGAgent is your GRU-based Actor-Critic)\n",
    "#     obs_dim = env.observation_space.shape[0]   # 4 + 4*window_size\n",
    "#     act_dim = env.action_space.n               # 3 (0: hold, 1: long, 2: short)\n",
    "#     agent = iRDPGAgent(obs_dim=obs_dim, action_dim=act_dim, hidden_dim=64, device=device)\n",
    "#     agent.to(device)\n",
    "\n",
    "#     replay_buffer = RBuffer(max_episodes=1000, device=device)\n",
    "#     # 3) Create the replay buffer\n",
    "#     for ep in expert_demos:\n",
    "#         replay_buffer.add_episode(ep)\n",
    "\n",
    "#     # 4) [Optional] Collect demonstration data from Dual Thrust.\n",
    "#     #    In your code, you had a function `collect_demonstrations`.\n",
    "#     #    We'll show how to use it. If you prefer a different demon policy,\n",
    "#     #    just adapt accordingly.\n",
    "\n",
    "#     # 5) Create separate optimizers for actor and critic\n",
    "#     actor_params = list(agent.actor_gru.parameters()) + list(agent.actor_fc.parameters())\n",
    "#     critic_params = list(agent.critic_gru.parameters()) + list(agent.critic_fc.parameters())\n",
    "\n",
    "#     actor_optimizer = torch.optim.Adam(actor_params, lr=lr_actor)\n",
    "#     critic_optimizer = torch.optim.Adam(critic_params, lr=lr_critic)\n",
    "\n",
    "#     # 6) Utility function for computing the RDPG losses\n",
    "#     #    We do \"backprop through time\" on entire episodes or truncated sub-sequences\n",
    "#     #    For simplicity, we'll do full-episode BPTT on small batch_size.\n",
    "#     def compute_losses(batch_eps):\n",
    "#         \"\"\"\n",
    "#         Compute actor and critic losses for a list of episodes (batch_eps).\n",
    "#         Each item is an Episode object with obs, actions, rewards, next_obs, done_flags.\n",
    "#         \"\"\"\n",
    "#         # We'll store MSE for critic, policy gradient for actor.\n",
    "#         # Because we have a recurrent architecture, we need to pass each\n",
    "#         # time step in sequence into the GRU.\n",
    "#         total_critic_loss = 0.0\n",
    "#         total_actor_loss = 0.0\n",
    "\n",
    "#         for ep in batch_eps:\n",
    "#             # (T, obs_dim)\n",
    "#             obs_seq = ep.obs\n",
    "#             actions_seq = ep.actions\n",
    "#             rewards_seq = ep.rewards\n",
    "#             done_seq = ep.done_flags\n",
    "\n",
    "#             T_steps = len(obs_seq)\n",
    "#             # We'll accumulate hidden states as we unroll the sequence\n",
    "#             h_actor = torch.zeros(1, 1, 64, device=device)   # (num_layers, batch=1, hidden_dim)\n",
    "#             h_critic = torch.zeros(1, 1, 64, device=device)\n",
    "\n",
    "#             # We collect all predicted Q-values, all chosen actions, etc.\n",
    "#             critic_loss = 0.0\n",
    "#             actor_loss = 0.0\n",
    "\n",
    "#             # We'll do discount factor manually or incorporate into reward\n",
    "#             # typical RDPG: we do a multi-step target with gamma\n",
    "#             # We'll keep track of the running return for the target\n",
    "#             # For simplicity, let's do a 1-step TD target (though multi-step is possible).\n",
    "#             for t in range(T_steps):\n",
    "#                 # Turn single obs into shape (1,1,obs_dim)\n",
    "#                 obs_t = obs_seq[t].view(1,1,-1)\n",
    "#                 # Pass into agent\n",
    "#                 with torch.no_grad():\n",
    "#                     # no_grad for deciding the action that was actually taken\n",
    "#                     dummy_action = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "#                     q_logits, q_value_all, h_actor_next, h_critic_next = agent.forward(\n",
    "#                         obs_t, dummy_action, h_actor, h_critic\n",
    "#                     )\n",
    "#                 # the action was discrete 0,1,2. Let's get the Q-value for that action:\n",
    "#                 act_idx = actions_seq[t]\n",
    "#                 # Q for the last time-step in the sequence dimension\n",
    "#                 # q_value_all is shape (1, action_dim) if we took the last index\n",
    "#                 Q_chosen = q_value_all[0, act_idx]\n",
    "\n",
    "#                 # to do 1-step TD, we need r + gamma * max Q(next)\n",
    "#                 # but iRDPG is typically deterministic, so let's do r + gamma * Q(next, a_next)\n",
    "#                 reward_t = rewards_seq[t]\n",
    "#                 done_t = done_seq[t]\n",
    "\n",
    "#                 # Next obs (if not done)\n",
    "#                 if t < (T_steps - 1) and not done_t:\n",
    "#                     next_obs_t = ep.obs[t+1].view(1,1,-1)\n",
    "#                     with torch.no_grad():\n",
    "#                         # get next action from actor\n",
    "#                         next_logits, _, h_actor_next2, h_critic_next2 = agent.forward(\n",
    "#                             next_obs_t, dummy_action, h_actor_next, h_critic_next\n",
    "#                         )\n",
    "#                         next_probs = torch.softmax(next_logits[0,0,:], dim=-1)\n",
    "#                         a_next_idx = next_probs.argmax().item()\n",
    "#                         # evaluate Q(next_obs, a_next)\n",
    "#                         # we need Q from the \"target\" network or from the same network with no_grad\n",
    "#                         # for simplicity, we'll reuse same agent as a target\n",
    "#                         _, q_value_next_all, _, _ = agent.forward(\n",
    "#                             next_obs_t, dummy_action, h_actor_next, h_critic_next\n",
    "#                         )\n",
    "#                         Q_next = q_value_next_all[0, a_next_idx]\n",
    "#                     td_target = reward_t + gamma * Q_next\n",
    "#                 else:\n",
    "#                     td_target = reward_t\n",
    "\n",
    "#                 # Critic loss: MSE\n",
    "#                 critic_loss_t = (td_target - Q_chosen)**2\n",
    "#                 critic_loss += critic_loss_t\n",
    "\n",
    "#                 # For actor loss, we want to move the actor's chosen action to maximize Q\n",
    "#                 # We'll do a second forward pass that requires grad for the actor\n",
    "#                 obs_t.requires_grad = True\n",
    "#                 q_logits_live, q_value_all_live, h_actor_live, h_critic_live = agent.forward(\n",
    "#                     obs_t, dummy_action, h_actor, h_critic\n",
    "#                 )\n",
    "#                 # The actor's chosen action is argmax of logits\n",
    "#                 a_probs = torch.softmax(q_logits_live[0,0,:], dim=-1)\n",
    "#                 # We do a “differentiable” version: the Q for the distribution of actions under the policy\n",
    "#                 # but iRDPG is typically picking a discrete action via argmax in inference.\n",
    "#                 # A simpler approach is to do a policy gradient that picks the best action w.r.t Q:\n",
    "#                 Q_actor = torch.sum(a_probs * q_value_all_live[0], dim=-1)\n",
    "#                 # We want to maximize Q_actor => minimize negative Q_actor\n",
    "#                 actor_loss_t = -Q_actor\n",
    "#                 actor_loss += actor_loss_t\n",
    "\n",
    "#                 # Move forward hidden states\n",
    "#                 h_actor = h_actor_next\n",
    "#                 h_critic = h_critic_next\n",
    "\n",
    "#             # Sum or average across timesteps\n",
    "#             critic_loss = critic_loss / T_steps\n",
    "#             actor_loss = actor_loss / T_steps\n",
    "\n",
    "#             total_critic_loss += critic_loss\n",
    "#             total_actor_loss += actor_loss\n",
    "\n",
    "#         # Average over episodes in the batch\n",
    "#         total_critic_loss /= len(batch_eps)\n",
    "#         total_actor_loss /= len(batch_eps)\n",
    "\n",
    "#         return total_critic_loss, total_actor_loss\n",
    "\n",
    "#     # 7) Main training loop\n",
    "#     print(\"Starting iRDPG training ...\")\n",
    "#     for ep_idx in range(n_train_episodes):\n",
    "#         # Collect 1 on-policy episode from the environment\n",
    "#         ep_data = Episode()\n",
    "#         obs = env.reset()\n",
    "#         done = False\n",
    "\n",
    "#         # You can add Gaussian or epsilon-greedy noise if needed\n",
    "#         while not done:\n",
    "#             # Decide action from agent\n",
    "#             # agent.act() expects (obs, h_actor) but for simplicity we track hidden states externally\n",
    "#             # or we can keep it in the agent. We'll do a simple approach:\n",
    "#             action, _ = agent.act(obs, torch.zeros(1,1,64,device=device))\n",
    "\n",
    "#             next_obs, reward, done, _info = env.step(action)\n",
    "\n",
    "#             ep_data.obs.append(obs)\n",
    "#             ep_data.actions.append(action)\n",
    "#             ep_data.rewards.append(reward)\n",
    "#             ep_data.done_flags.append(done)\n",
    "\n",
    "#             obs = next_obs\n",
    "\n",
    "#         # Store the newly collected episode\n",
    "#         replay_buffer.add_episode(ep_data)\n",
    "\n",
    "#         # Do some training updates\n",
    "#         for _ in range(train_iterations_per_episode):\n",
    "#             batch_eps = replay_buffer.sample(batch_size)\n",
    "#             critic_loss, actor_loss = compute_losses(batch_eps)\n",
    "\n",
    "#             # Backprop critic\n",
    "#             critic_optimizer.zero_grad()\n",
    "#             critic_loss.backward()\n",
    "#             critic_optimizer.step()\n",
    "\n",
    "#             # Backprop actor\n",
    "#             actor_optimizer.zero_grad()\n",
    "#             actor_loss.backward()\n",
    "#             actor_optimizer.step()\n",
    "\n",
    "#         if (ep_idx+1) % 10 == 0:\n",
    "#             print(f\"[Episode {ep_idx+1}/{n_train_episodes}] Critic Loss: {critic_loss.item():.4f}, \"\n",
    "#                   f\"Actor Loss: {actor_loss.item():.4f}\")\n",
    "\n",
    "#     print(\"Training complete!\")\n",
    "#     return agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_demos = collect_demonstrations(df, window_size=60, n_episodes=1)\n",
    "# torch.save(expert_demos, 'expert_demos.pt')\n",
    "# print(\"Expert demonstrations collected and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1_000\n",
    "\n",
    "# trained, target = train_iRDPG(df=df, window_size=60, train_episodes=t, batch_size=8, gamma=0.99, tau=0.01, expert_demos=expert_demos)\n",
    "\n",
    "trained_agent = train_iRDPG(df, n_train_episodes=t, n_demo_episodes=1, window_size=60, batch_size=8, train_iterations_per_episode=10, gamma=0.99, lr_actor=1e-4, lr_critic=1e-3, expert_demos=expert_demos, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
