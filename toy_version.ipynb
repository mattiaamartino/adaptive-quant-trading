{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gym import spaces\n",
    "from gym_trading_env.environments import TradingEnv\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/INTC_1Min_2023-08-01_2024-01-31.csv')\n",
    "df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDPTEnv(TradingEnv):\n",
    "    def __init__(self, df, window_size=60, initial_balance=100_000,transaction_cost=2.3e-5, slippage=0.2, eta=0.01, alpha=0, beta=0):\n",
    "        super().__init__(df=df)\n",
    "        \n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.slippage = slippage\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, \n",
    "            high=np.inf, \n",
    "            shape=(4 + 4 * window_size,), # OHLCV + 2 indicators + account\n",
    "            )\n",
    "        self.action_space = spaces.Discrete(3) # buy or sell\n",
    "\n",
    "        # Reward variables\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize\n",
    "        self.reset()\n",
    "        \n",
    "    def _compute_dual_thrust(self, k1=0.3, k2=0.3):\n",
    "        df = self.df.iloc[:self.current_step]\n",
    "        n = self.window_size\n",
    "\n",
    "        hh = df['high'].rolling(n).max().iloc[-1]\n",
    "        hc = df['close'].rolling(n).max().iloc[-1]\n",
    "        lc = df['close'].rolling(n).min().iloc[-1]\n",
    "        ll = df['low'].rolling(n).min().iloc[-1]\n",
    "\n",
    "        self.range = max(hh - lc, hc - ll)\n",
    "        self.buy_line = df['open'].iloc[-1] + k1 * self.range\n",
    "        self.sell_line = df['open'].iloc[-1] - k2 * self.range\n",
    "\n",
    "    def _compute_differential_sharpe_ratio(self, reward, eps=1e-6):\n",
    "        delta_alpha = reward - self.alpha\n",
    "        delta_beta = reward**2 - self.beta\n",
    "\n",
    "        if (self.beta - self.alpha**2) < eps: # prevent blow-up\n",
    "            dsr = 0\n",
    "        else:\n",
    "            dsr = (self.beta*delta_alpha - 0.5*self.alpha*delta_beta) / (self.beta - self.alpha**2)**1.5\n",
    "\n",
    "        # update\n",
    "        self.alpha += self.eta * delta_alpha\n",
    "        self.beta += self.eta * delta_beta\n",
    "\n",
    "        return dsr\n",
    "\n",
    "\n",
    "    def _next_observation(self):\n",
    "        prices = self.df.iloc[self.current_step - self.window_size:self.current_step][['open', 'high', 'low', 'close']].values.flatten()\n",
    "\n",
    "        self._compute_dual_thrust()\n",
    "        indicators = [self.buy_line, self.sell_line]\n",
    "\n",
    "        account = [self.position, self.balance/self.initial_balance]\n",
    "        return np.concatenate((prices, indicators, account))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0 # 0: no position, 1: long, -1: short\n",
    "        self.entry_price = 0\n",
    "        self.buy_line = 0\n",
    "        self.sell_line = 0\n",
    "\n",
    "        # first observation (update lines)\n",
    "        self._compute_dual_thrust()\n",
    "        return self._next_observation()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "            done = False\n",
    "            if self.current_step >= len(self.df) - 1:\n",
    "                done = True\n",
    "            \n",
    "            if action == 1:\n",
    "                desired_position = 1\n",
    "            elif action == 2:\n",
    "                desired_position = -1\n",
    "            else:\n",
    "                desired_position = self.position\n",
    "\n",
    "            price_open = self.df['open'].iloc[self.current_step]\n",
    "            \n",
    "            # Close hold open new\n",
    "            if desired_position != self.position:\n",
    "                # close old\n",
    "                if self.position != 0:\n",
    "                    old_pnl = (price_open - self.entry_price) * self.position\n",
    "                    cost = (abs(self.position - desired_position) * self.transaction_fee * price_open \n",
    "                            + abs(self.position)*self.slippage)\n",
    "                    self.balance += old_pnl - cost\n",
    "                \n",
    "                # open new\n",
    "                if desired_position != 0:\n",
    "                    self.entry_price = price_open\n",
    "                    cost = (abs(self.position - desired_position) * self.transaction_fee * price_open\n",
    "                            + abs(desired_position)*self.slippage)\n",
    "                    self.balance -= cost\n",
    "                \n",
    "                self.position = desired_position\n",
    "            \n",
    "            # next step\n",
    "            self.current_step += 1\n",
    "            if self.current_step < len(self.df):\n",
    "                self._compute_dual_thrust()\n",
    "            \n",
    "            price_close = self.df['close'].iloc[self.current_step - 1]\n",
    "            step_pnl = (price_close - self.entry_price) * self.position\n",
    "            \n",
    "            dsr = self._compute_differential_sharpe_ratio(step_pnl)\n",
    "            obs = self._next_observation()\n",
    "            \n",
    "            if self.balance <= 0:\n",
    "                done = True\n",
    "            \n",
    "            return obs, dsr, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = POMDPTEnv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_policy(env):\n",
    "\n",
    "    o = env._next_observation()\n",
    "    n = env.window_size\n",
    "\n",
    "    buy_line = o[4*n]\n",
    "    sell_line= o[4*n + 1]\n",
    "\n",
    "    curr = env.df['open'].iloc[env.current_step-1]\n",
    "\n",
    "    if curr > buy_line:\n",
    "        return 1\n",
    "    elif curr < sell_line:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "def intraday_greedy_actions(env_df, window_size=60, device=\"cuda\"):\n",
    "\n",
    "    num_steps = len(env_df)\n",
    "    day_len = 240  \n",
    "\n",
    "    open_prices = torch.tensor(env_df[\"open\"].values, dtype=torch.float32, device=device)\n",
    "    actions = torch.zeros(num_steps, dtype=torch.int, device=device)\n",
    "\n",
    "    i = window_size\n",
    "    while i < (num_steps - 1):\n",
    "        day_start = i\n",
    "        day_end = min(i + day_len, num_steps)\n",
    "        \n",
    "        day_opens = open_prices[day_start:day_end]\n",
    "        idx_min = torch.argmin(day_opens).item()  # Buy\n",
    "        idx_max = torch.argmax(day_opens).item()  # Sell\n",
    "\n",
    "        actions[day_start + idx_min] = 1  # Long\n",
    "        actions[day_start + idx_max] = 2  # Short\n",
    "\n",
    "        i = day_end  \n",
    "\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_obs = []\n",
    "        self.done_flags = []\n",
    "        self.expert_actions = []  # 'prophetic' (intraday actions)\n",
    "\n",
    "class RBuffer:\n",
    "    def __init__(self, max_episodes=1000, device=\"cuda\"):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.device = device\n",
    "        self.episodes = []\n",
    "    \n",
    "    def add_episode(self, ep):\n",
    "        if len(self.episodes) >= self.max_episodes:\n",
    "            self.episodes.pop(0)\n",
    "\n",
    "        ep.obs = torch.tensor(ep.obs, dtype=torch.float32, device=self.device)\n",
    "        ep.actions = torch.tensor(ep.actions, dtype=torch.int64, device=self.device)\n",
    "        ep.rewards = torch.tensor(ep.rewards, dtype=torch.float32, device=self.device)\n",
    "        ep.next_obs = torch.tensor(ep.next_obs, dtype=torch.float32, device=self.device)\n",
    "        ep.done_flags = torch.tensor(ep.done_flags, dtype=torch.bool, device=self.device)\n",
    "        self.episodes.append(ep)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = torch.randint(0, len(self.episodes), (batch_size,), device=self.device)\n",
    "        batch_eps = [self.episodes[i] for i in indices.cpu().numpy()]\n",
    "        return batch_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iRDPGAgent(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim=3, hidden_dim=64, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor_gru = nn.GRU(obs_dim, hidden_dim, batch_first=True) # [batch, seq_len, obs_dim]\n",
    "        self.actor_fc = nn.Linear(hidden_dim, action_dim) # [batch, seq_len, action_dim]\n",
    "\n",
    "        self.critic_gru = nn.GRU(obs_dim + action_dim, hidden_dim, batch_first=True) # [batch, seq_len, obs_dim + action_dim]\n",
    "        self.critic_fc = nn.Linear(hidden_dim, action_dim) # [batch, seq_len, action_dim]\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, obs, h_actor, h_critic):\n",
    "\n",
    "        obs = obs.to(self.device)\n",
    "        # Actor\n",
    "        z_actor, h_actor = self.actor_gru(obs, h_actor)\n",
    "        logits = self.actor_fc(z_actor)\n",
    "        # Critic\n",
    "        z_critic, h_critic = self.critic_gru(obs, h_critic)\n",
    "        q_value = self.critic_fc(z_critic[:, -1])\n",
    "\n",
    "        return logits, q_value, h_actor, h_critic\n",
    "    \n",
    "    def act(self, obs, h_actor):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).view(1,1,-1)  \n",
    "            logits, _, h_actor, _ = self.forward(obs_t, h_actor)\n",
    "            probs = torch.softmax(logits[0,0,:], dim=-1)  \n",
    "            action = probs.argmax().item()\n",
    "            \n",
    "        return action, h_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(env, agent, noise):\n",
    "\n",
    "    ep = Episode()\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float, device=device).view(1,1,-1)\n",
    "            logits, Q_vals, _, _ = agent(obs_t)\n",
    "            # pick a policy action\n",
    "            probs = torch.softmax(logits[0,0,:], dim=-1)\n",
    "            if noise:\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                action = dist.sample().item()\n",
    "            else:\n",
    "                action = probs.argmax().item()\n",
    "        \n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        ep.obs.append(obs)\n",
    "        ep.actions.append(action)\n",
    "        ep.rewards.append(reward)\n",
    "        ep.next_obs.append(next_obs)\n",
    "        ep.done_flags.append(done)\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    return ep\n",
    "\n",
    "def collect_demonstrations(df, window_size=60, n_episodes=50):\n",
    "    demos = []\n",
    "    env = POMDPTEnv(df, window_size=window_size)\n",
    "    for _ in trange(n_episodes, desc=\"Collecting Demonstrations\"):\n",
    "        ep = Episode()\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = dt_policy(env)\n",
    "            next_obs, rew, done, _ = env.step(a)\n",
    "            ep.obs.append(obs)\n",
    "            ep.actions.append(a)\n",
    "            ep.rewards.append(rew)\n",
    "            ep.next_obs.append(next_obs)\n",
    "            ep.done_flags.append(done)\n",
    "            obs = next_obs\n",
    "        demos.append(ep)\n",
    "    return demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iRDPG(df, window_size=60, hidden_dim=64,\n",
    "                train_episodes=500, batch_size=8, gamma=0.99,\n",
    "                tau=0.01, expert_demos=None):\n",
    "    \n",
    "    env = POMDPTEnv(df, window_size=window_size)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n\n",
    "    \n",
    "    agent = iRDPGAgent(obs_dim, act_dim, hidden_dim).to(device)\n",
    "    target_agent = iRDPGAgent(obs_dim, act_dim, hidden_dim).to(device)\n",
    "    target_agent.load_state_dict(agent.state_dict())\n",
    "    \n",
    "    # Create a replay buffer\n",
    "    buffer = RBuffer(max_episodes=1000)\n",
    "    print(f\"Initialized the environment, agent and replay buffer.\")\n",
    "    \n",
    "    # Collect some demonstration episodes\n",
    "    for ep in expert_demos:\n",
    "        buffer.add_episode(ep)\n",
    "    print(f\"Collected {len(expert_demos)} demonstration episodes.\")\n",
    "    \n",
    "    # Create \"prophetic\" array for entire df:\n",
    "    prophecy = intraday_greedy_actions(df, window_size=window_size, device=device)\n",
    "    print(f\"Created the 'prophetic' array for the entire dataset.\")\n",
    "    \n",
    "    # Attach them to each Episode so we can do Q-filter BC later\n",
    "    for ep in buffer.episodes:\n",
    "        ep.expert_actions = []\n",
    "        # naive mapping: the environment index starts at window_size for step=0\n",
    "        # so step i -> global index = window_size + i\n",
    "        idx_global = window_size\n",
    "        for t in range(len(ep.obs)):\n",
    "            if idx_global < len(prophecy):\n",
    "                ep.expert_actions.append(prophecy[idx_global])\n",
    "            else:\n",
    "                ep.expert_actions.append(0)\n",
    "            idx_global += 1\n",
    "    print(f\"Attached the 'prophetic' actions to demonstration episodes.\")\n",
    "    \n",
    "    optimizer = nn.optim.Adam(agent.parameters(), lr=1e-3)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def soft_update(net, net_targ, tau):\n",
    "        for p, p_targ in zip(net.parameters(), net_targ.parameters()):\n",
    "            p_targ.data.copy_(tau * p.data + (1.0 - tau) * p_targ.data)\n",
    "    \n",
    "    def update_agent(batch_eps):\n",
    "        \"\"\"\n",
    "        Unroll entire episodes on GPU for BPTT.\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        agent.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for ep in tqdm(batch_eps, desc=\"Updating agent\"):\n",
    "            obs_seq = torch.tensor(ep.obs, dtype=torch.float, device=device)\n",
    "            actions_seq = torch.tensor(ep.actions, dtype=torch.long, device=device)\n",
    "            rewards_seq = torch.tensor(ep.rewards, dtype=torch.float, device=device)\n",
    "            exp_seq = torch.tensor(ep.expert_actions, dtype=torch.long, device=device)\n",
    "            \n",
    "            # shape => [1, T, obs_dim]\n",
    "            obs_seq = obs_seq.unsqueeze(0) \n",
    "            \n",
    "            logits, Q_vals, _, _ = agent(obs_seq)  # [1,T,act_dim]\n",
    "            logits = logits[0]  # [T,act_dim]\n",
    "            Q_vals = Q_vals[0] # [T,act_dim]\n",
    "            T_len = len(actions_seq)\n",
    "            \n",
    "            # Critic loss: basic 1-step TD\n",
    "            td_errors = []\n",
    "            for t in range(T_len):\n",
    "                q_sa = Q_vals[t, actions_seq[t]]\n",
    "                if t < T_len-1 and not ep.done_flags[t]:\n",
    "                    # next action from agent policy\n",
    "                    next_a = logits[t+1].argmax().item()\n",
    "                    q_next = Q_vals[t+1, next_a].detach()\n",
    "                    target = rewards_seq[t] + gamma * q_next\n",
    "                else:\n",
    "                    target = rewards_seq[t]\n",
    "                td_errors.append(target - q_sa)\n",
    "            \n",
    "            td_errors = torch.stack(td_errors)\n",
    "            critic_loss = (td_errors**2).mean()\n",
    "            \n",
    "            # Actor loss: - Q(s, a_pi)\n",
    "            pi = torch.softmax(logits, dim=-1)\n",
    "            Q_expected = (pi * Q_vals).sum(dim=-1)\n",
    "            actor_loss = - Q_expected.mean()\n",
    "            \n",
    "            # Behavior cloning with Q-filter\n",
    "            a_agent = logits.argmax(dim=-1)\n",
    "            Q_expert = Q_vals[torch.arange(T_len), exp_seq]\n",
    "            Q_agent  = Q_vals[torch.arange(T_len), a_agent]\n",
    "            mask = (Q_expert > Q_agent).float()\n",
    "            \n",
    "            bc_loss_t = ce(logits, exp_seq)  # cross-entropy stepwise\n",
    "            bc_loss   = (bc_loss_t * mask).mean()\n",
    "            \n",
    "            ep_loss = critic_loss + actor_loss + bc_loss\n",
    "            ep_loss.backward(retain_graph=True)\n",
    "            total_loss += ep_loss.item()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        soft_update(agent, target_agent, tau)\n",
    "        \n",
    "        return total_loss / len(batch_eps)\n",
    "    \n",
    "    # main training loop\n",
    "    update_trange = trange(train_episodes, desc=\"Training\")\n",
    "    for ep_i in update_trange:\n",
    "        new_ep = collect_episode(env, agent=agent, noise=True)\n",
    "        # attach prophecy to new episode\n",
    "        new_ep.expert_actions = []\n",
    "        idx_global = window_size\n",
    "        for t in range(len(new_ep.obs)):\n",
    "            if idx_global < len(prophecy):\n",
    "                new_ep.expert_actions.append(prophecy[idx_global])\n",
    "            else:\n",
    "                new_ep.expert_actions.append(0)\n",
    "            idx_global += 1\n",
    "        \n",
    "        buffer.add_episode(new_ep)\n",
    "        \n",
    "        if len(buffer) >= batch_size:\n",
    "            batch_eps = buffer.sample(batch_size)\n",
    "            loss_val = update_agent(batch_eps)\n",
    "        else:\n",
    "            loss_val = 0.0\n",
    "        update_trange.set_postfix(loss=loss_val)\n",
    "        \n",
    "        print(f\"Episode {ep_i+1}/{train_episodes}, Loss={loss_val:.4f}\")\n",
    "    \n",
    "    return agent, target_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_demos = collect_demonstrations(df, window_size=60, n_episodes=50)\n",
    "torch.save(torch.tensor(expert_demos), 'expert_demos.pt')\n",
    "print(\"Expert demonstrations collected and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized the environment, agent and replay buffer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Demonstrations:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Demonstrations:   0%|          | 0/50 [04:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10_000\u001b[39m\n\u001b[0;32m----> 3\u001b[0m trained, target \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iRDPG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mtrain_iRDPG\u001b[0;34m(df, window_size, hidden_dim, train_episodes, batch_size, gamma, tau)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized the environment, agent and replay buffer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Collect some demonstration episodes\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m demo_episodes \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_demonstrations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m demo_episodes:\n\u001b[1;32m     20\u001b[0m     buffer\u001b[38;5;241m.\u001b[39madd_episode(ep)\n",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m, in \u001b[0;36mcollect_demonstrations\u001b[0;34m(df, window_size, n_episodes)\u001b[0m\n\u001b[1;32m     38\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 40\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mdt_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     next_obs, rew, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m     42\u001b[0m     ep\u001b[38;5;241m.\u001b[39mobs\u001b[38;5;241m.\u001b[39mappend(obs)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mdt_policy\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdt_policy\u001b[39m(env):\n\u001b[0;32m----> 3\u001b[0m     o \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     n \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mwindow_size\n\u001b[1;32m      6\u001b[0m     buy_line \u001b[38;5;241m=\u001b[39m o[\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mn]\n",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m, in \u001b[0;36mPOMDPTEnv._next_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_observation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m     prices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_dual_thrust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     indicators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuy_line, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msell_line]\n\u001b[1;32m     61\u001b[0m     account \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbalance\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_balance]\n",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mPOMDPTEnv._compute_dual_thrust\u001b[0;34m(self, k1, k2)\u001b[0m\n\u001b[1;32m     27\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step]\n\u001b[1;32m     28\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size\n\u001b[0;32m---> 30\u001b[0m hh \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhigh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     31\u001b[0m hc \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(n)\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     32\u001b[0m lc \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(n)\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/env_rl/lib64/python3.9/site-packages/pandas/core/window/rolling.py:2168\u001b[0m, in \u001b[0;36mRolling.max\u001b[0;34m(self, numeric_only, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   2134\u001b[0m     template_header,\n\u001b[1;32m   2135\u001b[0m     create_section_header(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2167\u001b[0m ):\n\u001b[0;32m-> 2168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_rl/lib64/python3.9/site-packages/pandas/core/window/rolling.py:1581\u001b[0m, in \u001b[0;36mRollingAndExpandingMixin.max\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_apply(sliding_min_max, engine_kwargs, is_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1580\u001b[0m window_func \u001b[38;5;241m=\u001b[39m window_aggregations\u001b[38;5;241m.\u001b[39mroll_max\n\u001b[0;32m-> 1581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_rl/lib64/python3.9/site-packages/pandas/core/window/rolling.py:619\u001b[0m, in \u001b[0;36mBaseWindow._apply\u001b[0;34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_columnwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_tablewise(homogeneous_func, name, numeric_only)\n",
      "File \u001b[0;32m~/env_rl/lib64/python3.9/site-packages/pandas/core/window/rolling.py:472\u001b[0m, in \u001b[0;36mBaseWindow._apply_columnwise\u001b[0;34m(self, homogeneous_func, name, numeric_only)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_numeric_only(name, numeric_only)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj, numeric_only)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# GH 12541: Special case for count where we support date-like types\u001b[39;00m\n",
      "File \u001b[0;32m~/env_rl/lib64/python3.9/site-packages/pandas/core/window/rolling.py:456\u001b[0m, in \u001b[0;36mBaseWindow._apply_series\u001b[0;34m(self, homogeneous_func, name)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo numeric types to aggregate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mhomogeneous_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_axis_for_step(obj\u001b[38;5;241m.\u001b[39mindex, result)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor(result, index\u001b[38;5;241m=\u001b[39mindex, name\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/env_rl/lib64/python3.9/site-packages/pandas/core/window/rolling.py:614\u001b[0m, in \u001b[0;36mBaseWindow._apply.<locals>.homogeneous_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x, start, end, min_periods, \u001b[38;5;241m*\u001b[39mnumba_args)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 614\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcalc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/env_rl/lib64/python3.9/site-packages/pandas/core/window/rolling.py:611\u001b[0m, in \u001b[0;36mBaseWindow._apply.<locals>.homogeneous_func.<locals>.calc\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    602\u001b[0m start, end \u001b[38;5;241m=\u001b[39m window_indexer\u001b[38;5;241m.\u001b[39mget_window_bounds(\n\u001b[1;32m    603\u001b[0m     num_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(x),\n\u001b[1;32m    604\u001b[0m     min_periods\u001b[38;5;241m=\u001b[39mmin_periods,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m     step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep,\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_window_bounds(start, end, \u001b[38;5;28mlen\u001b[39m(x))\n\u001b[0;32m--> 611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnumba_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = 10_000\n",
    "\n",
    "trained, target = train_iRDPG(df=df, window_size=60, train_episodes=t, batch_size=8, gamma=0.99, tau=0.01, expert_demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_policy(agent, env, n_episodes=5, render=False):\n",
    "    \"\"\"\n",
    "    Runs a given agent's policy for n_episodes in the given environment\n",
    "    and returns some evaluation metrics.\n",
    "    \n",
    "    Returns:\n",
    "        - average_final_balance\n",
    "        - average_total_pnl\n",
    "        - list of final balances for each episode\n",
    "        - list of step-by-step balances (for optional analysis)\n",
    "    \"\"\"\n",
    "    agent.eval()  # put the network in eval mode\n",
    "    \n",
    "    initial_balance = env.initial_balance\n",
    "    final_balances = []\n",
    "    step_balances_all = []\n",
    "    \n",
    "    for ep_i in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        # Keep track of the environment's balance at each step\n",
    "        step_balances = [env.balance]\n",
    "        \n",
    "        while not done:\n",
    "            # In the differential Sharpe environment, the environment's \"reward\"\n",
    "            # is not the usual PnL—so we track actual positions/balance ourselves.\n",
    "            with torch.no_grad():\n",
    "                # shape = [1,1,obs_dim]\n",
    "                obs_t = torch.tensor(obs, dtype=torch.float).view(1,1,-1)\n",
    "                # forward pass through agent\n",
    "                logits, Q_vals, _, _ = agent(obs_t)\n",
    "                probs = torch.softmax(logits[0,0,:], dim=-1)\n",
    "                action = probs.argmax().item()  # greedy action\n",
    "                \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            step_balances.append(env.balance)\n",
    "            \n",
    "            if render:\n",
    "                print(f\"Step: {env.current_step}, Action: {action}, Balance: {env.balance:.2f}\")\n",
    "        \n",
    "        final_balances.append(env.balance)\n",
    "        step_balances_all.append(step_balances)\n",
    "    \n",
    "    final_balances = np.array(final_balances)\n",
    "    avg_final_balance = final_balances.mean()\n",
    "    avg_total_pnl = (final_balances - initial_balance).mean()\n",
    "    \n",
    "    print(f\"Evaluation over {n_episodes} episodes:\")\n",
    "    print(f\"  Average Final Balance: {avg_final_balance:.2f}\")\n",
    "    print(f\"  Average Total PnL:     {avg_total_pnl:.2f}\")\n",
    "    \n",
    "    return avg_final_balance, avg_total_pnl, final_balances, step_balances_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
